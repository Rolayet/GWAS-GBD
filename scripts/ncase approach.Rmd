---
title: "Example analysis of Rayan's project"
output: html_notebook
---


Read in the data

```{r}
# Load necessary libraries

library(dplyr)
library(data.table)
library(tidyr)
library(stringr)
library(DescTools)
library(ggplot2)
library(stats)
library(readr)
library(ontologyIndex)
library(readxl)
library(openxlsx)
library(boot)
library(LorenzRegression)


# a <- fread("~/Downloads/gwas_catalog_v1.0.2.1-studies_r2024-03-11.tsv")

a <- fread("/Users/rayanaloliet/Desktop/GWAS Catalog/gwas_catalog_v1.0.2.1-studies_r2024-06-07.csv")
 

```


create a new column name ncase to sum up the number of participants from initial and replicate sample size. 

Example of Processing the sample size

Remove commas:
Input: "25,453 European ancestry cases, 58,113 European ancestry controls, 360 cases and controls, PMID:25056061, 6,524 cases, 24,001 controls"
Result: "25453 European ancestry cases 58113 European ancestry controls 360 cases and controls PMID:25056061 6524 cases 24001 controls"

Remove phrases like "and controls":
Input: "25453 European ancestry cases 58113 European ancestry controls 360 cases and controls PMID:25056061 6524 cases 24001 controls"
Regex: gsub("\band\s*controls?\b", "", text, ignore.case = TRUE)
Result: "25453 European ancestry cases 58113 European ancestry controls 360 cases PMID:25056061 6524 cases 24001 controls"

Remove standalone "controls" with preceding numbers:
Input: "25453 European ancestry cases 58113 European ancestry controls 360 cases PMID:25056061 6524 cases 24001 controls"
Regex: gsub("\b\d+\s*controls?\b", "", text, ignore.case = TRUE)
Result: "25453 European ancestry cases 360 cases PMID:25056061 6524 cases"

Remove any remaining PubMed ID references:
Input: "25453 European ancestry cases 360 cases PMID:25056061 6524 cases"
Regex: gsub("\bPMID:\d+\b", "", text, ignore.case = TRUE)
Result: "25453 European ancestry cases 360 cases 6524 cases"

Extract and sum remaining numbers:
Input: "25453 European ancestry cases 360 cases 6524 cases"
Regex: str_extract_all(text, "\b\d+\b") %>% unlist() %>% as.numeric()
Numbers extracted: 25453, 360, 6524
Sum: 25453 + 360 + 6524 = 32337

```{r}

# Function to exclude control numbers, PubMed IDs, and sum remaining participant numbers
extract_and_sum_excluding_nonparticipants <- function(text) {
  # Remove commas
  text <- gsub(",", "", text)
  # Remove phrases like "and controls"
  text <- gsub("\\band\\s*controls?\\b", "", text, ignore.case = TRUE)
  # Remove standalone "controls" with preceding numbers
  text <- gsub("\\b\\d+\\s*controls?\\b", "", text, ignore.case = TRUE)
  # Remove any remaining PMID references
  text <- gsub("\\bPMID:\\d+\\b", "", text, ignore.case = TRUE)
  # Extract and sum remaining numbers
  numbers <- str_extract_all(text, "\\b\\d+\\b") %>% unlist() %>% as.numeric()
  sum(numbers, na.rm = TRUE)
}

# Apply the function to create new numeric columns for cases only, excluding controls and PubMed IDs
a <- a %>%
  mutate(
    INITIAL_SAMPLE_SIZE_CASES = sapply(`INITIAL SAMPLE SIZE`, extract_and_sum_excluding_nonparticipants),
    REPLICATION_SAMPLE_SIZE_CASES = sapply(`REPLICATION SAMPLE SIZE`, extract_and_sum_excluding_nonparticipants)
  )

# Create ncase column, summing up cases only and excluding controls and PubMed IDs
a <- a %>%
  mutate(ncase = coalesce(INITIAL_SAMPLE_SIZE_CASES, 0) + coalesce(REPLICATION_SAMPLE_SIZE_CASES, 0))

# summary
summary(a$ncase)

```


Have a look at the data

```{r}
hist(a$`ASSOCIATION COUNT`, breaks=100)
```

Sometimes there is more than one EFO term (comma separated). Split them into separate lines

```{r}

head(a, 10)

a <- a %>%
    tidyr::separate_rows(MAPPED_TRAIT_URI, sep = ", ")

head(a, 10)

```

Get the number of EFO terms per publication

```{r}
pubmed_count <- a %>% group_by(PUBMEDID) %>% 
    summarise(
        n_efo = length(unique(MAPPED_TRAIT_URI))
    ) %>% arrange(desc(n_efo))

pubmed_count

 
```

Add the EFO count per PUBMEDID to the data frame

```{r}
a <- left_join(a, pubmed_count, by="PUBMEDID")
str(a)
```


Removing extra whitespace, cleaning the Impact factor values and ensure the values are numeric 

```{r}
# Remove extra whitespace
a$`Impact factor` <- trimws(a$`Impact factor`)

# Replace non-standard characters
a$`Impact factor` <- gsub("Â·", ".", a$`Impact factor`)

# Convert to numeric, forcing non-numeric values to NA (for those with no impact factors)
a$`Impact factor` <- as.numeric(a$`Impact factor`)

# Replace NA values with 0
a$`Impact factor`[is.na(a$`Impact factor`)] <- 0

# Verify the changes
str(a)
unique(a$`Impact factor`)
summary(a$`Impact factor`)


```



Select the study with the largest ncase for each MAPPED_TRAIT_URI 

```{r}
best_studies <- a %>%
  group_by(MAPPED_TRAIT_URI) %>%
  filter(ncase == max(ncase)) %>%
  ungroup()
```


Develop attention score including ncase 

```{r}


attention <- best_studies %>%
  group_by(MAPPED_TRAIT_URI) %>%
  summarise(
    total_ncase = sum(ncase),
    weighted_n = sum(1 / n_efo),
    nhits = sum(`ASSOCIATION COUNT`),
    weighted_nhits = sum(`ASSOCIATION COUNT` / n_efo),
    weighted_attention_score_impact_factor = sum((1 / n_efo) * `Impact factor`, na.rm = TRUE),
     DISEASE_TRAIT = first(`DISEASE/TRAIT`),
    PUBMEDID = paste(unique(PUBMEDID), collapse = ", ")
  )
str(attention)

```
 
Step 2: Merging the GWAS attention scores with the GBD disease burden results. The GBD terms were split into first and second parts because the terms in the second part have descendants. Therefore, those in the second part need to be handled differently, where we have to pull out the descendants from the tree via ontologyIndex library and then do the matching.


```{r}

#First_part_GBD<- fread("/Users/rayanaloliet/Desktop/GWAS Catalog/GBD.csv")
First_part_GBD<- read_xlsx("/Users/rayanaloliet/Desktop/GWAS Catalog/First_part_GBD.xlsx")

str(First_part_GBD)

  # unique matched rows  by GBD term
First_part_GBD<- First_part_GBD%>%
  distinct(`GBD term`, .keep_all = TRUE)

 
```


Specify columns that contain EFO terms

```{r}

efo_columns <- c("EFO 1", "EFO 2", "EFO 3", "EFO 4", "EFO 5", "EFO 6", 
                 "EFO 7", "EFO 8", "EFO 9", "EFO 10", "EFO 11", "EFO 12",
                 "EFO 13", "EFO 14", "EFO 15", "EFO 16", "EFO 17", "EFO 18",
                 "EFO 19", "EFO 20", "EFO 21", "EFO 22", "EFO 23", "EFO 24",
                 "EFO 25", "EFO 26", "EFO 27", "EFO 28", "EFO 29", "EFO 30")
```


Reshape the GBD data to gather all relevant columns into key-value pairs and drop rows with NA EFO terms

```{r}

 gbd_long <- First_part_GBD%>%
pivot_longer(cols = all_of(efo_columns), names_to = "EFO_number", values_to =  "MAPPED_TRAIT_URI") %>% drop_na(MAPPED_TRAIT_URI) 

gbd_long
```

Separate rows based on a separator (assuming comma-separated EFO terms) and Trim whitespace

```{r}
gbd_long <- gbd_long %>%
  separate_rows(MAPPED_TRAIT_URI, sep = ",") %>%
  mutate(MAPPED_TRAIT_URI = trimws(MAPPED_TRAIT_URI))  

gbd_long
```

Ensure consistent formatting, cleaning and standardize MAPPED_TRAIT_URI in gbd_long (GBD file)

*There are many functions here because each time I attempted to find a matching, I was unable to do so due to the differences in the two datasets. For example, in GWAS, the identifiers have underscores, while in GBD they have colons, and so on... Therefore, the functions below are intended to unify these differences. 

```{r}

# Ensure consistent formatting
gbd_long$MAPPED_TRAIT_URI <- tolower(trimws(gbd_long$MAPPED_TRAIT_URI))

# Ensure lowercase
gbd_long$MAPPED_TRAIT_URI <- tolower(gbd_long$MAPPED_TRAIT_URI) 

# Trim leading/trailing whitespace
gbd_long$MAPPED_TRAIT_URI <- trimws(gbd_long$MAPPED_TRAIT_URI) 

# Remove all spaces
gbd_long$MAPPED_TRAIT_URI <- gsub("\\s+", "", gbd_long$MAPPED_TRAIT_URI) 

# Remove all internal spaces explicitly
gbd_long$MAPPED_TRAIT_URI <- gsub(" ", "", gbd_long$MAPPED_TRAIT_URI) 

# Convert to lowercase and remove spaces 
gbd_long$MAPPED_TRAIT_URI <- tolower(gsub("\\s+", "", gbd_long$MAPPED_TRAIT_URI)) 

# Remove colons and spaces
gbd_long$MAPPED_TRAIT_URI <- gsub("[:\\s]", "", gbd_long$MAPPED_TRAIT_URI)  

# Remove quotation marks
gbd_long$MAPPED_TRAIT_URI <- gsub("\"", "", gbd_long$MAPPED_TRAIT_URI) 

# Replace _ with : 
gbd_long$MAPPED_TRAIT_URI <- gsub("_", ":", gbd_long$MAPPED_TRAIT_URI) 

# Remove all extra spaces
gbd_long$MAPPED_TRAIT_URI <- str_squish(gbd_long$MAPPED_TRAIT_URI) 

(head(gbd_long$MAPPED_TRAIT_URI, 20))

```

Ensure consistent formatting, cleaning and standardize MAPPED_TRAIT_URI in attention (GWAS file) and remove rows with NA in MAPPED_TRAIT_URI (EFO terms)

```{r}
# Function to clean and standardize MAPPED_TRAIT_URI
clean_mapped_trait_uri <- function(uri) {
  # Convert to lowercase
  uri <- tolower(uri)
  # Trim leading/trailing whitespace
  uri <- trimws(uri)
  # Remove all internal spaces explicitly
  uri <- gsub("\\s+", "", uri)
  # Remove quotation marks
  uri <- gsub("\"", "", uri)
  # Replace _ with :
  uri <- gsub("_", ":", uri)
  # Remove colons and spaces
  uri <- gsub("[:\\s]", "", uri)
  # Remove prefixes and ensure consistency
  uri <- sub(".*[:/]", "", uri)
  return(uri)
}

# Apply the function to MAPPED_TRAIT_URI column
attention$MAPPED_TRAIT_URI <- clean_mapped_trait_uri(attention$MAPPED_TRAIT_URI)


# Convert "na" strings to actual NA values
attention$MAPPED_TRAIT_URI[attention$MAPPED_TRAIT_URI == "na"] <- NA

# Remove rows with NA in MAPPED_TRAIT_URI
attention <- attention %>%
  filter(!is.na(MAPPED_TRAIT_URI))

(head(attention$MAPPED_TRAIT_URI, 20))

```

Finding a match between GWAS and GBD (First_part_GBD) via identifiers (EFO terms)- for those with no descendants. Then grouping those terms by GBD term and PUBMEDID (as we need to ensure the studies are independent when we sum the n ).  


```{r}

# Merge the data frames
merged_data <- merge(attention, gbd_long, by = "MAPPED_TRAIT_URI", all.x = TRUE)

# Identify matched rows
matched_rows <- inner_join(attention, gbd_long, by = "MAPPED_TRAIT_URI")

 
 # Group by GBD term  and summarize within these groups 
 unique_matched_gbd_terms_First_part_GBD <- matched_rows %>%
    group_by(`GBD term`) %>%
    summarise(
      total_ncase = sum(total_ncase),
      weighted_n = first(weighted_n),
      nhits = first(nhits),
      weighted_nhits = first(weighted_nhits),
      weighted_attention_score_impact_factor = first(weighted_attention_score_impact_factor),
      MAPPED_TRAIT_URI = first(MAPPED_TRAIT_URI),
      DISEASE_TRAIT = first(DISEASE_TRAIT),
      .groups = "drop"
    )
 
   
# Identify unmatched rows from attention
unmatched_attention <- anti_join(attention, gbd_long, by = "MAPPED_TRAIT_URI")

# Identify unmatched rows from gbd_long
unmatched_gbd <- anti_join(gbd_long, attention, by = "MAPPED_TRAIT_URI")

# Ensure unique unmatched_gbd rows by GBD term
unmatched_gbd_unique <- unmatched_gbd %>%
  distinct(`GBD term`, .keep_all = TRUE)

# Extract unique GBD terms for matched and unmatched entries
matched_gbd_terms <- unique_matched_gbd_terms_First_part_GBD$`GBD term` %>% unique()
unmatched_gbd_terms <- unmatched_gbd_unique$`GBD term` %>% unique()

# Remove any GBD terms from unmatched_gbd that are also in matched_rows
unique_unmatched_gbd_terms <- setdiff(unmatched_gbd_terms, matched_gbd_terms)

# Ensure unique unmatched_gbd rows by GBD term
unique_unmatched_gbd_terms_First_part_GBD <- unmatched_gbd_unique %>%
  filter(`GBD term` %in% unique_unmatched_gbd_terms)

head(unique_matched_gbd_terms_First_part_GBD)

# Save the data frames to Excel files
#write.xlsx(unique_matched_gbd_terms_First_part_GBD, "unique_matched_gbd_terms_First_part_GBD.xlsx")
#write.xlsx(unique_unmatched_gbd_terms_First_part_GBD, "unique_unmatched_gbd_terms_First_part_GBD.xlsx") 

```

```{r}

# Define utility functions
format_identifier <- function(identifier) {
  formatted <- gsub(" ", "", identifier) # Remove any spaces
  formatted <- toupper(formatted) # Convert to uppercase
  formatted <- gsub("([A-Z]+)([0-9]+)", "\\1:\\2", formatted) # Add colon
  return(formatted)
}

revert_identifier <- function(identifier) {
  reverted <- gsub(":", "", identifier) # Remove colons
  return(tolower(reverted)) # Convert to lowercase
}

clean_mapped_trait_uri <- function(uri) {
  uri <- tolower(uri) # Convert to lowercase
  uri <- trimws(uri) # Trim leading/trailing whitespace
  uri <- gsub("\\s+", "", uri) # Remove all internal spaces explicitly
  uri <- gsub("\"", "", uri) # Remove quotation marks
  uri <- gsub("_", ":", uri) # Replace _ with :
  uri <- gsub("[:\\s]", "", uri) # Remove colons and spaces
  uri <- sub(".*[:/]", "", uri) # Remove prefixes and ensure consistency
  return(uri)
}

# Read data files
ontology <- get_OBO("/Users/rayanaloliet/Desktop/GWAS Catalog/efo-obo.txt")
Second_part_GBD <- read_excel("/Users/rayanaloliet/Desktop/GWAS Catalog/Second_part_GBD.xlsx")


# Extract and combine identifiers from Second_part_GBD
identifiers_list <- list(
  Second_part_GBD$`MAPPED_TRAIT_URI...2`,
  Second_part_GBD$`MAPPED_TRAIT_URI...3`,
  Second_part_GBD$`MAPPED_TRAIT_URI...4`,
  Second_part_GBD$`MAPPED_TRAIT_URI...5`,
  Second_part_GBD$`MAPPED_TRAIT_URI...6`,
  Second_part_GBD$`MAPPED_TRAIT_URI...7`,
  Second_part_GBD$`MAPPED_TRAIT_URI...8`,
  Second_part_GBD$`MAPPED_TRAIT_URI...9`,
  Second_part_GBD$`MAPPED_TRAIT_URI...10`,
  Second_part_GBD$`MAPPED_TRAIT_URI...11`,
  Second_part_GBD$`MAPPED_TRAIT_URI...12`,
  Second_part_GBD$`MAPPED_TRAIT_URI...13`
)
identifiers <- do.call(c, lapply(identifiers_list, na.omit))

# Format the identifiers
formatted_identifiers <- sapply(identifiers, format_identifier)

# Extract descendants for each identifier
descendants_list <- list()
for (i in 1:length(formatted_identifiers)) {
  id <- formatted_identifiers[i]
  if (id %in% ontology$id) {
    descendants <- get_descendants(ontology, id)
    descendants_list[[id]] <- descendants
  } else {
    descendants_list[[id]] <- NA
  }
}

# Determine the maximum number of descendants
max_descendants <- max(sapply(descendants_list, function(x) if (is.null(x) || all(is.na(x))) 0 else length(x)), na.rm = TRUE)

# Create a data frame for descendants
descendants_df <- data.frame(MAPPED_TRAIT_URI = character(), stringsAsFactors = FALSE)
for (i in 1:max_descendants) {
  descendants_df[paste0("Descendant_", i)] <- character()
}

# Populate the data frame with identifiers and their descendants
for (i in seq_along(formatted_identifiers)) {
  id <- formatted_identifiers[i]
  descendants <- descendants_list[[id]]
  if (is.null(descendants) || all(is.na(descendants))) {
    row <- c(MAPPED_TRAIT_URI = id, rep("", max_descendants))
  } else {
    row <- c(MAPPED_TRAIT_URI = id, descendants, rep("", max_descendants - length(descendants)))
  }
  
  row_df <- as.data.frame(t(row), stringsAsFactors = FALSE)
  colnames(row_df) <- colnames(descendants_df)
  descendants_df <- rbind(descendants_df, row_df)
}

# Convert columns to character type
descendants_df[] <- lapply(descendants_df, as.character)

# Add the GBD_TERM column
gbd_terms <- Second_part_GBD$`GBD term`
descendants_df$GBD_TERM <- gbd_terms[match(descendants_df$MAPPED_TRAIT_URI, formatted_identifiers)]

# Reorder columns to place GBD_TERM after MAPPED_TRAIT_URI
if ("GBD_TERM" %in% colnames(descendants_df)) {
  descendants_df <- descendants_df %>% select(MAPPED_TRAIT_URI, GBD_TERM, everything())
}

# Revert the identifiers to their original format
descendants_df <- descendants_df %>% mutate(across(c(MAPPED_TRAIT_URI, starts_with("Descendant_")), ~sapply(., revert_identifier)))

# Ensure identifiers are formatted correctly in attention
attention <- attention %>% mutate(MAPPED_TRAIT_URI = format_identifier(MAPPED_TRAIT_URI))

# Remove duplicates in both data frames
attention <- attention %>% distinct(MAPPED_TRAIT_URI, .keep_all = TRUE)
descendants_df <- descendants_df %>% distinct(MAPPED_TRAIT_URI, .keep_all = TRUE)

# Reshape descendants_df to long format
descendants_long <- descendants_df %>%
  pivot_longer(cols = starts_with("Descendant_"), names_to = "Descendant", values_to = "Descendant_URI") %>%
  filter(Descendant_URI != "") %>%
  select(MAPPED_TRAIT_URI, GBD_TERM, Descendant_URI)

# Clean and standardize Descendant_URI
descendants_long$Descendant_URI <- tolower(trimws(descendants_long$Descendant_URI))
descendants_long$Descendant_URI <- gsub("\\s+", "", descendants_long$Descendant_URI)
descendants_long$Descendant_URI <- gsub(" ", "", descendants_long$Descendant_URI)
descendants_long$Descendant_URI <- gsub("[:\\s]", "", descendants_long$Descendant_URI)
descendants_long$Descendant_URI <- gsub("\"", "", descendants_long$Descendant_URI)
descendants_long$Descendant_URI <- gsub("_", ":", descendants_long$Descendant_URI)
descendants_long$Descendant_URI <- str_squish(descendants_long$Descendant_URI)
descendants_long <- descendants_long %>% distinct(Descendant_URI, .keep_all = TRUE)

# Clean and standardize MAPPED_TRAIT_URI in attention
attention$MAPPED_TRAIT_URI <- clean_mapped_trait_uri(attention$MAPPED_TRAIT_URI)

# Identify matched entries
matched_entries <- attention %>%
  filter(MAPPED_TRAIT_URI %in% descendants_long$Descendant_URI) %>%
  left_join(descendants_long %>% select(Descendant_URI, GBD_TERM), by = c("MAPPED_TRAIT_URI" = "Descendant_URI"))

  
   # Group by GBD term  and summarize within these groups 
 unique_matched_gbd_terms_Second_part_GBD <- matched_entries %>%
    group_by(GBD_TERM) %>%
    summarise(
      total_ncase = sum(total_ncase),
      weighted_n = first(weighted_n),
      nhits = first(nhits),
      weighted_nhits = first(weighted_nhits),
      weighted_attention_score_impact_factor = first(weighted_attention_score_impact_factor),
      MAPPED_TRAIT_URI = first(MAPPED_TRAIT_URI),
      DISEASE_TRAIT = first(DISEASE_TRAIT),
      .groups = "drop"
    )
  
# Identify unmatched GBD entries based on GBD_TERM in both datasets
# First, extract unique GBD terms from both datasets
matched_gbd_terms <- matched_entries$GBD_TERM %>% unique()
unmatched_gbd_terms <- Second_part_GBD$`GBD term` %>% unique()

# Identify unmatched GBD terms
unique_unmatched_gbd_terms <- setdiff(unmatched_gbd_terms, matched_gbd_terms)

# Filter the Second_part_GBD dataframe to get rows with these unique unmatched GBD terms
unique_unmatched_gbd_terms_Second_part_GBD <- Second_part_GBD %>%
  filter(`GBD term` %in% unique_unmatched_gbd_terms)

# Save the data frames to Excel files
#write.xlsx(unique_unmatched_gbd_terms_Second_part_GBD, unique_unmatched_gbd_terms_Second_part_GBD.xlsx")
#write.xlsx(unique_matched_gbd_terms_Second_part_GBD, "unique_matched_gbd_terms_Second_part_GBD.xlsx")
```

```{r}
# List of columns to keep for unmatched terms
columns_unmatched <- c("GBD term", "EFO_number", "MAPPED_TRAIT_URI")

# Select only the necessary columns from the first unmatched data frame
unique_unmatched_gbd_terms_First_part_GBD <- unique_unmatched_gbd_terms_First_part_GBD[, columns_unmatched]

# Add missing columns to the second unmatched data frame
if (!"EFO_number" %in% names(unique_unmatched_gbd_terms_Second_part_GBD)) {
  unique_unmatched_gbd_terms_Second_part_GBD$EFO_number <- NA
}

# Rename the appropriate column in the second unmatched data frame
names(unique_unmatched_gbd_terms_Second_part_GBD)[which(names(unique_unmatched_gbd_terms_Second_part_GBD) == "MAPPED_TRAIT_URI...2")] <- "MAPPED_TRAIT_URI"

# Select and order the columns to match the first unmatched data frame
unique_unmatched_gbd_terms_Second_part_GBD <- unique_unmatched_gbd_terms_Second_part_GBD[, columns_unmatched]

# Combine the unmatched terms
combined_unmatched_terms <- rbind(unique_unmatched_gbd_terms_First_part_GBD, unique_unmatched_gbd_terms_Second_part_GBD)

# Display the results
print("Combined Unmatched Terms:")
print(combined_unmatched_terms)


# List of columns to keep for matched terms
columns_matched <- c("MAPPED_TRAIT_URI", "total_ncase", "weighted_n", "nhits", "weighted_nhits", "weighted_attention_score_impact_factor", "GBD term", "DISEASE_TRAIT")

# Select only the necessary columns from the first matched data frame
unique_matched_gbd_terms_First_part_GBD <- unique_matched_gbd_terms_First_part_GBD[, columns_matched]

# Rename the column in the second matched data frame
names(unique_matched_gbd_terms_Second_part_GBD)[which(names(unique_matched_gbd_terms_Second_part_GBD) == "GBD_TERM")] <- "GBD term"

# Ensure all columns are present in the second matched data frame, filling with NA where necessary
for (col in columns_matched) {
  if (!col %in% names(unique_matched_gbd_terms_Second_part_GBD)) {
    unique_matched_gbd_terms_Second_part_GBD[[col]] <- NA
  }
}

# Select and order the columns to match the first matched data frame
unique_matched_gbd_terms_Second_part_GBD <- unique_matched_gbd_terms_Second_part_GBD[, columns_matched]

# Combine the matched terms
combined_matched_terms <- rbind(unique_matched_gbd_terms_First_part_GBD, unique_matched_gbd_terms_Second_part_GBD)

# Display the results
print("Combined Matched Terms:")
print(combined_matched_terms)

# Remove NA in combined_matched_terms 
combined_matched_terms <- combined_matched_terms %>%
  drop_na()

#write.xlsx(combined_matched_terms, combined_matched_terms.xlsx")
#write.xlsx(combined_unmatched_terms, "combined_unmatched_terms.xlsx")

```

```{r}
# Create a new column for disease names to facilitate partial matching in attention
attention <- attention %>%
  mutate(MAPPED_TRAIT_NAME = tolower(trimws(gsub("[^a-zA-Z0-9 ]", "", DISEASE_TRAIT))))

# Create a new column for disease names to facilitate partial matching in combined_unmatched_terms
combined_unmatched_terms <- combined_unmatched_terms%>%
  mutate(GBD_TERM_NAME = tolower(trimws(gsub("[^a-zA-Z0-9 ]", "", combined_unmatched_terms$`GBD term`))))


# Perform partial matching between MAPPED_TRAIT_NAME in attention and GBD_TERM_NAME in combined_unmatched_terms with the rest of the information 

partial_matches <- combined_unmatched_terms %>%
  cross_join(attention) %>%
  filter(str_detect(MAPPED_TRAIT_NAME, GBD_TERM_NAME)) %>%
  select(
    GBD_TERM_NAME, 
    MAPPED_TRAIT_NAME, 
    GBD_TRAIT_URI = MAPPED_TRAIT_URI.x, 
    MAPPED_TRAIT_URI = MAPPED_TRAIT_URI.y,
    total_ncase,
    weighted_n, 
    nhits, 
    weighted_nhits, 
    weighted_attention_score_impact_factor,
    `GBD term`
  ) %>%
  distinct()
 
     # Now summarize these attention scores across the GBD term and sum the total_attention_score_per_pubmed 
  partial_matches_unique <- partial_matches %>%
    group_by(`GBD term`) %>%
    summarise(
      total_ncase = sum(total_ncase, na.rm = TRUE),
      weighted_n = first(weighted_n),
      nhits = first(nhits),
      weighted_nhits = first(weighted_nhits),
      weighted_attention_score_impact_factor = first(weighted_attention_score_impact_factor),
      MAPPED_TRAIT_URI = first(MAPPED_TRAIT_URI),
      GBD_TRAIT_URI = first(GBD_TRAIT_URI),
      MAPPED_TRAIT_NAME = first(MAPPED_TRAIT_NAME),
      .groups = "drop"
    )

```


```{r}
# Identify the rows that were matched partially
matched_partial_gbd <- partial_matches_unique %>% 
  select(GBD_TRAIT_URI, `GBD term`) %>% 
  distinct() 

# Get final unmatched rows
final_unmatched_gbd <- anti_join(combined_unmatched_terms, matched_partial_gbd, by = "GBD term")


# Check for duplicates in final_unmatched_gbd
duplicates_final_unmatched_gbd <- final_unmatched_gbd[duplicated(final_unmatched_gbd$`GBD term`), ]
if (nrow(duplicates_final_unmatched_gbd) > 0) {
  print("Duplicates found in final_unmatched_gbd:")
  print(duplicates_final_unmatched_gbd)
} else {
  print("No duplicates found in final_unmatched_gbd.")
}


# final matched_gbd (combine those identified via Identifier and Partially)
final_matched_gbd <- bind_rows(
  combined_matched_terms %>% mutate(Match_Type = "Identifier"),
  partial_matches_unique %>% mutate(Match_Type = "Partial")
)

# Check for duplicates in final matched_gbd
duplicates_final_matched_gbd <- final_matched_gbd[duplicated(final_matched_gbd$`GBD term`), ]
if (nrow(duplicates_final_matched_gbd) > 0) {
  print("Duplicates found in final_matched_gbd:")
  print(duplicates_final_matched_gbd)
} else {
  print("No duplicates found in final_matched_gbd")
}


# Save the results to a CSV file
#write.csv(final_unmatched_gbd, "final_unmatched_gbd.csv", row.names = FALSE)
#write.csv(final_matched_gbd, "final_matched_gbd.csv", row.names = FALSE)


# Rename the column 'total_ncase' to 'total_attention_score'
final_matched_gbd <- final_matched_gbd %>%
  rename(total_attention_score = total_ncase)

```

```{r}

#  Add 'total_attention_score' column with value 0 to final_unmatched_gbd
final_unmatched_gbd <- final_unmatched_gbd %>%
  mutate(total_attention_score = 0)

#  Ensure both datasets have the same columns
# Here, we ensure that final_unmatched_gbd has all necessary columns
required_columns <- names(final_matched_gbd)

# Add any missing columns to final_unmatched_gbd and set their values to NA
for (col in required_columns) {
  if (!(col %in% names(final_unmatched_gbd))) {
    final_unmatched_gbd[[col]] <- NA
  }
}

# Combine the datasets
combined_dataset <- bind_rows(final_matched_gbd, final_unmatched_gbd)

combined_dataset <- combined_dataset %>%
  distinct(`GBD term`, .keep_all = TRUE)


# Save the combined dataset to a CSV file
#write.csv(combined_dataset, "combined_dataset.csv", row.names = FALSE)

```


```{r}
# Load the GBD DALY data
gbd_daly_data <- read_csv('/Users/rayanaloliet/Desktop/GWAS Catalog/IHME-GBD_2021_DATA-120ebfcd-1.csv')

# Filter out rows with NA values in the key columns
gbd_daly_data <- gbd_daly_data %>%
  filter(!is.na(`cause_name`), !is.na(val))

# Verify that there are no NA values left
sum(is.na(gbd_daly_data$`cause_name`))
sum(is.na(gbd_daly_data$val))
sum(is.na(gbd_daly_data$measure_name))

 
# Inspect the structure of the GBD DALY data
str(gbd_daly_data)

```


```{r}
#2019 
#gbd_daly_data <- gbd_daly_data %>%
 # filter(metric_name == "Number" & measure_name == "DALYs (Disability-Adjusted Life Years)") #%>%
 # rename(`GBD term`= cause_name, DALY = val)
#2021
gbd_daly_data <- gbd_daly_data %>%
  filter(metric_name == "Number") %>%
  rename(`GBD term`= cause_name, DALY = val)


# Merge the DALY values into the combined dataset
combined_dataset <- combined_dataset %>%
  left_join(gbd_daly_data, by = c("GBD term" = "GBD term"))

# Remove specified columns
combined_dataset <- combined_dataset %>%
  select(-c(DISEASE_TRAIT, GBD_TRAIT_URI, MAPPED_TRAIT_NAME, EFO_number, GBD_TERM_NAME))

# Verify the merge
head(combined_dataset)

# Ensure the columns are numeric as it gives error massage 
combined_dataset$total_attention_score <- as.numeric(combined_dataset$total_attention_score)
combined_dataset$DALY <- as.numeric(combined_dataset$DALY)

# Check for any NA values and handle them
combined_dataset[is.na(combined_dataset$DALY), 'DALY'] <- 0

# Verify the data
summary(combined_dataset$total_attention_score)
summary(combined_dataset$DALY)

```


```{r}
unmatched_gbd_terms <- anti_join(gbd_daly_data, combined_dataset, by = c("GBD term" = "GBD term"))

unmatched_gbd_terms <- unmatched_gbd_terms %>%
  distinct(`GBD term`, .keep_all = TRUE)

# Get the unique GBD terms in each dataset
combined_gbd_terms <- unique(combined_dataset$`GBD term`)
gbd_gbd_terms <- unique(gbd_daly_data$`GBD term`)

# Identify terms in combined_dataset not in gbd_daly_data
terms_only_in_combined <- setdiff(combined_gbd_terms, gbd_gbd_terms)

# Identify terms in gbd_daly_data not in combined_dataset
terms_only_in_gbd_daly <- setdiff(gbd_gbd_terms, combined_gbd_terms)

# Get the lengths of the unique terms
len_combined <- length(terms_only_in_combined)
len_gbd_daly <- length(terms_only_in_gbd_daly)

# Determine the maximum length to ensure both columns in the data frame have the same number of rows
max_length <- max(len_combined, len_gbd_daly)

# Combine the differences into a data frame for better visualization
diff_2 <- data.frame(
  Terms_Only_in_Combined_Dataset = c(terms_only_in_combined, rep(NA, max_length - len_combined)),
  Terms_Only_in_GBD_Daly_Dataset = c(terms_only_in_gbd_daly, rep(NA, max_length - len_gbd_daly))
)

# the differences
(diff_2)

#write.csv(diff_2, "diff_2.csv", row.names = FALSE)

```

```{r}
# Visualize the distribution of 'total_attention_score'
ggplot(combined_dataset, aes(x = total_attention_score)) +
  geom_histogram(binwidth = 500, fill = 'blue', color = 'black', alpha = 0.7) +
  labs(title = "Histogram of Total Attention Scores", x = "total_attention_score", y = "Frequency") +
  xlim(c(0, 35000))  


# Adding 1 to avoid log(0) which is undefined, for better visualization 
combined_dataset$total_attention_score_log <- log10(combined_dataset$total_attention_score + 1)

# Visualize the distribution with log transformation
ggplot(combined_dataset, aes(x = total_attention_score_log)) +
  geom_histogram(binwidth = 0.1, fill = 'blue', color = 'black', alpha = 0.7) +
  labs(title = "Histogram of Log-Transformed Total Attention Scores (log(n + 1))", 
       x = "Log10(total attention Score + 1)", 
       y = "Frequency") +
  scale_x_continuous(breaks = seq(0, log10(max(combined_dataset$total_attention_score + 1)), by = 1))


# Boxplot for 'total_attention_score'
ggplot(combined_dataset, aes(y = total_attention_score)) +
  geom_boxplot(fill = 'blue', color = 'black') +
  labs(title = "Boxplot of Total Attention Scores (n)", y = "Total Attention Score (n)")

```


```{r}
# Create the histogram for DALY distribution
ggplot(combined_dataset, aes(x = DALY)) +
  geom_histogram(binwidth = 10000, aes(fill = after_stat(count)), color = 'black', alpha = 0.7) +
  scale_fill_gradient(low = "red", high = "darkred") +
  labs(title = "Histogram of DALY", x = "DALY", y = "Frequency") +
  theme_minimal()


# Boxplot for 'DALY'
ggplot(combined_dataset, aes(y = DALY)) +
  geom_boxplot(fill = 'red', color = 'black') +
  labs(title = "Boxplot of DALY", y = "DALY")

```

```{r}
# Scatter plot to visualize the relationship between 'total attention score' and 'DALY'
ggplot(combined_dataset, aes(x = DALY, y = total_attention_score)) +
  geom_point(color = 'purple') +
  labs(title = "Scatter Plot of DALY vs Total Attention Scores (n)", x = "DALY", y = "Total Attention Score (n)")

# Scatter plot to visualize the relationship between 'weighted_attention_score_impact_factor' and 'DALY'
filtered_dataset <- combined_dataset %>%
  filter(!is.na(DALY) & !is.na(weighted_attention_score_impact_factor))

ggplot(filtered_dataset, aes(x = DALY, y = weighted_attention_score_impact_factor)) +
  geom_point(color = 'blue') +
  labs(title = "Scatter Plot of DALY vs Weighted Attention Score Impact Factor", 
       x = "DALY", 
       y = "Weighted Attention Score Impact Factor")
```


```{r}
# check the summary of the total_attention_score column 
summary(combined_dataset$total_attention_score)

# Check the count of rows where total_attention_score is zero
sum(combined_dataset$total_attention_score == 0)


# Filter out rows with zero total attention scores
filtered_dataset <- combined_dataset %>% filter(total_attention_score > 0)

# Check the summary of the filtered data
summary(filtered_dataset$total_attention_score)


# Summary of 'total_attention_score' in the filtered dataset
summary(filtered_dataset$total_attention_score)

```


```{r}
# Histogram of Attention Scores (n)
ggplot(filtered_dataset, aes(x = total_attention_score)) +
  geom_histogram(binwidth = 1, fill = 'blue', color = 'black', alpha = 0.7) +
  labs(title = "Histogram of Attention Scores (n) - Filtered", x = "Attention Score (n)", y = "Frequency")

```


```{r}
# Filter for diseases with high DALY but low attention scores
threshold_attention = 1  # Define what is considered low attention
high_daly_low_attention <- combined_dataset %>%
  filter(DALY > median(DALY) & total_attention_score <= threshold_attention) %>%
  arrange(desc(DALY))

# View the top diseases with high DALY and low attention
(head(high_daly_low_attention, 20))

# Save the filtered data to a CSV file
#write.csv(high_daly_low_attention, "high_daly_low_attention.csv", row.names = FALSE)


```

```{r}
summary(combined_dataset$total_attention_score)
frequency <- table(combined_dataset$total_attention_score)


specific_value <- 33796

# Filter the dataset for the specific total_attention_score_per_pubmed value
filtered_rows <- combined_dataset %>%
  filter(total_attention_score == specific_value)

# Extract unique GBD terms
unique_gbd_terms <- unique(filtered_rows$`GBD term`)

# Print the unique GBD terms
print(unique_gbd_terms)
```

```{r}
# Sort the data frame by total_attention_score in descending order
sorted_data <- combined_dataset[order(combined_dataset$total_attention_score, decreasing = TRUE), ]

# Remove duplicates based on total_attention_score and GBD term combination
unique_data <- sorted_data[!duplicated(sorted_data[, c("total_attention_score", "GBD term")]), ]

# Select the top 50 unique values
top_50_scores <- head(unique_data, 50)

# Display the top 50 scores
print(top_50_scores)


# Threshold value
threshold_value <- 198

# Filter the dataset for total_attention_score_per_pubmed values greater than the threshold
filtered_rows <- combined_dataset %>%
  filter(total_attention_score > threshold_value)

# Extract unique GBD terms
unique_gbd_terms <- unique(filtered_rows$`GBD term`)

# Print the unique GBD terms
print(unique_gbd_terms)

# save
#write.csv(unique_gbd_terms, file = "unique_gbd_terms.csv", row.names = FALSE)
```

```{r}
# Sort the data frame by total_attention_score in descending order and remove duplicates based on total_attention_score and GBD term combination
unique_data <- combined_dataset %>%
  arrange(desc(total_attention_score)) %>%
  distinct(total_attention_score, `GBD term`, .keep_all = TRUE)

# Rank the DALY values based on the deduplicated dataset
unique_data <- unique_data %>%
  mutate(DALY_rank = rank(DALY))

# Threshold value
threshold_value <- 198

# Filter the dataset for total_attention_score values greater than the threshold
filtered_rows <- unique_data %>%
  filter(total_attention_score > threshold_value)

# Summarize the data to get frequency and list of unique total_attention_score and DALY for each GBD term
summary_table <- filtered_rows %>%
  group_by(`GBD term`) %>%
  summarise(
    total_attention_score = unique(total_attention_score),
    DALY = unique(DALY),
    DALY_rank = unique(DALY_rank),
    .groups = "drop"
  )

# View the summary table
print(summary_table)
```

```{r}
location_names <- unique(combined_dataset$location_name)
valid_data_threshold <- 10  # Define a threshold for valid data points

concentration_results <- data.frame(
  location_name = character(),
  concentration_index = numeric(),
  ci_lower = numeric(),
  ci_upper = numeric(),
  total_points = numeric(),
  valid_points = numeric(),
  stringsAsFactors = FALSE
)

locations_failed <- c()
locations_passed <- c()

# Define the concentration index calculation function
concentration_index_function <- function(data, indices) {
  # Resample data
  resampled_data <- data[indices, ]
  
  # Sort the data by attention scores
  sorted_indices <- order(resampled_data$attention_scores)
  sorted_attention_scores <- resampled_data$attention_scores[sorted_indices]
  sorted_daly_values <- resampled_data$daly_values[sorted_indices]
  
  # Calculate cumulative proportions
  cumulative_attention_scores <- cumsum(sorted_attention_scores) / sum(sorted_attention_scores)
  cumulative_daly_values <- cumsum(sorted_daly_values) / sum(sorted_daly_values)
  
  # Calculate the concentration index
  concentration_index <- 2 * sum(cumulative_daly_values * sorted_attention_scores) / sum(sorted_attention_scores) - 1
  
  return(concentration_index)
}

for (location in location_names) {
  cat("Processing location:", location, "\n")
  
  subset_data <- combined_dataset %>%
    filter(location_name == !!location)
  
  total_attention_scores <- subset_data$total_attention_score
  daly_values <- subset_data$DALY
  
  # Remove rows with missing values
  valid_data <- subset_data %>%
    filter(!is.na(total_attention_scores) & !is.na(daly_values))
  
  total_attention_scores <- valid_data$total_attention_score
  daly_values <- valid_data$DALY
  
  # Check for non-positive values
  valid_points <- nrow(valid_data)
  total_points <- nrow(subset_data)
  
  if (valid_points < valid_data_threshold) {
    cat("Skipping location:", location, "- not enough valid data points.\n")
    cat("Total points:", total_points, "- Valid points:", valid_points, "\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  data <- data.frame(attention_scores = total_attention_scores, daly_values = daly_values)
  
  # Check for zero variance in data
  if (var(total_attention_scores) == 0 || var(daly_values) == 0) {
    cat("Zero variance in data for location:", location, "\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  # Print the data distribution for debugging
  cat("Total Attention Scores - Min:", min(total_attention_scores), "Max:", max(total_attention_scores), "Mean:", mean(total_attention_scores), "\n")
  cat("DALY Values - Min:", min(daly_values), "Max:", max(daly_values), "Mean:", mean(daly_values), "\n")
  
  set.seed(123)  # for reproducibility
  bootstrap_results <- tryCatch({
    boot(data, concentration_index_function, R = 1000)  # Increase the number of replications if needed
  }, error = function(e) {
    cat("Error in bootstrapping for location:", location, "\n")
    return(NULL)
  })
  
  if (is.null(bootstrap_results)) {
    cat("Skipping location:", location, "due to bootstrapping error.\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  bootstrap_ci <- tryCatch({
    boot.ci(bootstrap_results, type = "perc")
  }, error = function(e) {
    cat("Error in confidence interval calculation for location:", location, "\n")
    return(NULL)
  })
  
  if (is.null(bootstrap_ci)) {
    cat("Skipping location:", location, "due to confidence interval calculation error.\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  concentration_index <- concentration_index_function(data, 1:nrow(data))
  
  concentration_results <- concentration_results %>%
    add_row(
      location_name = location,
      concentration_index = concentration_index,
      ci_lower = bootstrap_ci$percent[4],
      ci_upper = bootstrap_ci$percent[5],
      total_points = total_points,
      valid_points = valid_points
    )
  
  # Calculate cumulative proportions for plotting the concentration curve
  sorted_indices <- order(total_attention_scores)
  cumulative_attention_scores <- cumsum(total_attention_scores[sorted_indices]) / sum(total_attention_scores)
  cumulative_daly_values <- cumsum(daly_values[sorted_indices]) / sum(daly_values)
  
  # Plot the concentration curve
  plot(cumulative_daly_values, cumulative_attention_scores, type="l", col="blue", lwd=2, 
       xlab="Cumulative Proportion of DALY Values", ylab="Cumulative Proportion of Attention Scores",
       main=paste("Concentration Curve -", location))
  abline(0, 1, col="red", lwd=2, lty=2) # Line of equality
  
  cat("Displayed plot for location:", location, "\n")
  locations_passed <- c(locations_passed, location)
}

# Print summary of results
print(concentration_results)
cat("Locations passed:", paste(locations_passed, collapse = ", "), "\n")
cat("Locations failed:", paste(locations_failed, collapse = ", "), "\n")

# Combined concentration curve for all data
all_attention_scores <- combined_dataset$total_attention_score
all_daly_values <- combined_dataset$DALY

# Remove rows with missing values
valid_data_all <- combined_dataset %>%
  filter(!is.na(total_attention_score) & !is.na(DALY))

all_attention_scores <- valid_data_all$total_attention_score
all_daly_values <- valid_data_all$DALY

# Prepare data for the combined plot
all_data <- data.frame(attention_scores = all_attention_scores, daly_values = all_daly_values)

# Calculate the concentration index and bootstrap CI for all data
all_concentration_index <- concentration_index_function(all_data, 1:nrow(all_data))

set.seed(123)  # for reproducibility
all_bootstrap_results <- boot(all_data, concentration_index_function, R = 1000)

# Calculate the 95% confidence interval for the combined data
all_bootstrap_ci <- boot.ci(all_bootstrap_results, type = "perc")

# Calculate cumulative proportions for plotting the combined concentration curve
sorted_indices_all <- order(all_attention_scores)
cumulative_attention_scores_all <- cumsum(all_attention_scores[sorted_indices_all]) / sum(all_attention_scores)
cumulative_daly_values_all <- cumsum(all_daly_values[sorted_indices_all]) / sum(all_daly_values)

# Plot the combined concentration curve
plot(cumulative_daly_values_all, cumulative_attention_scores_all, type="l", col="blue", lwd=2, 
     xlab="Cumulative Proportion of DALY Values", ylab="Cumulative Proportion of Attention Scores",
     main="Combined Concentration Curve")
abline(0, 1, col="red", lwd=2, lty=2) # Line of equality

# Print the combined concentration index and its confidence interval
cat("Combined Concentration Index:", all_concentration_index, "\n")
cat("95% Confidence Interval:", all_bootstrap_ci$percent[4], "-", all_bootstrap_ci$percent[5], "\n")


# Remove rows with NA values
concentration_results_clean <- na.omit(concentration_results)

# Plot concentration index with 95% confidence intervals for all locations individually and combined
ggplot(concentration_results_clean, aes(x = reorder(location_name, concentration_index), y = concentration_index)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
  coord_flip() +
  labs(
    title = "Concentration Index by Location",
    x = "Location",
    y = "Concentration Index"
  ) +
  theme_minimal()

```

```{r}
# List of terms to exclude
terms_to_exclude <- c("All causes",
                      "Cirrhosis due to alcohol",
                      "Low back pain",
                      "Neck pain",
                      "Cocaine use disorders",
                      "Drug use disorders",
                      "Foreign body in other body part",
                      "Opioid use disorders",
                      "Poisonings",
                      "Falls",
                      "Foreign body",
                      "Conflict and terrorism",
                      "Drowning",
                      "Poisoning by carbon monoxide",
                      "Extensively drug-resistant tuberculosis",
                      "Venomous animal contact",
                      "Sexual violence",
                      "Adverse effects of medical treatment",
                      "Pulmonary aspiration and foreign body in airway",
                      "Motorcyclist road injuries",
                      "Motor vehicle road injuries",
                      "Physical violence by firearm",
                      "Physical violence by sharp object",
                      "Environmental heat and cold exposure",
                      "Police conflict and executions",
                      "Exposure to forces of nature",
                      "Animal contact",
                      "Non-venomous animal contact",
                      "Fire, heat, and hot substances",
                      "Unintentional firearm injuries",
                      "Indirect maternal deaths",
                      "Late maternal deaths",
                      "Amphetamine use disorders",
                      "Other drug use disorders",
                      "Other infectious diseases",
                      "Other transport injuries",
                      "Unintentional injuries",
                      "Other unintentional injuries",
                      "Poisoning by other means",
                      "Exposure to mechanical forces",
                      "Physical violence by other means",
                      "Injuries",
                      "Transport injuries",
                      "Road injuries",
                      "Other road injuries",
                      "Foreign body in eyes",
                      "Cannabis use disorders")

# Filter the GBD DALY data for "Number" metric and rename columns
combined_dataset <- combined_dataset %>%
  filter(!(`GBD term` %in% terms_to_exclude))  # Exclude specified terms


```


```{r}
location_names <- unique(combined_dataset$location_name)
valid_data_threshold <- 10  # Define a threshold for valid data points

concentration_results <- data.frame(
  location_name = character(),
  concentration_index = numeric(),
  ci_lower = numeric(),
  ci_upper = numeric(),
  total_points = numeric(),
  valid_points = numeric(),
  stringsAsFactors = FALSE
)

locations_failed <- c()
locations_passed <- c()

# Define the concentration index calculation function
concentration_index_function <- function(data, indices) {
  # Resample data
  resampled_data <- data[indices, ]
  
  # Sort the data by attention scores
  sorted_indices <- order(resampled_data$attention_scores)
  sorted_attention_scores <- resampled_data$attention_scores[sorted_indices]
  sorted_daly_values <- resampled_data$daly_values[sorted_indices]
  
  # Calculate cumulative proportions
  cumulative_attention_scores <- cumsum(sorted_attention_scores) / sum(sorted_attention_scores)
  cumulative_daly_values <- cumsum(sorted_daly_values) / sum(sorted_daly_values)
  
  # Calculate the concentration index
  concentration_index <- 2 * sum(cumulative_daly_values * sorted_attention_scores) / sum(sorted_attention_scores) - 1
  
  return(concentration_index)
}

for (location in location_names) {
  cat("Processing location:", location, "\n")
  
  subset_data <- combined_dataset %>%
    filter(location_name == !!location)
  
  total_attention_scores <- subset_data$total_attention_score
  daly_values <- subset_data$DALY
  
  # Remove rows with missing values
  valid_data <- subset_data %>%
    filter(!is.na(total_attention_scores) & !is.na(daly_values))
  
  total_attention_scores <- valid_data$total_attention_score
  daly_values <- valid_data$DALY
  
  # Check for non-positive values
  valid_points <- nrow(valid_data)
  total_points <- nrow(subset_data)
  
  if (valid_points < valid_data_threshold) {
    cat("Skipping location:", location, "- not enough valid data points.\n")
    cat("Total points:", total_points, "- Valid points:", valid_points, "\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  data <- data.frame(attention_scores = total_attention_scores, daly_values = daly_values)
  
  # Check for zero variance in data
  if (var(total_attention_scores) == 0 || var(daly_values) == 0) {
    cat("Zero variance in data for location:", location, "\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  # Print the data distribution for debugging
  cat("Total Attention Scores - Min:", min(total_attention_scores), "Max:", max(total_attention_scores), "Mean:", mean(total_attention_scores), "\n")
  cat("DALY Values - Min:", min(daly_values), "Max:", max(daly_values), "Mean:", mean(daly_values), "\n")
  
  set.seed(123)  # for reproducibility
  bootstrap_results <- tryCatch({
    boot(data, concentration_index_function, R = 1000)  # Increase the number of replications if needed
  }, error = function(e) {
    cat("Error in bootstrapping for location:", location, "\n")
    return(NULL)
  })
  
  if (is.null(bootstrap_results)) {
    cat("Skipping location:", location, "due to bootstrapping error.\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  bootstrap_ci <- tryCatch({
    boot.ci(bootstrap_results, type = "perc")
  }, error = function(e) {
    cat("Error in confidence interval calculation for location:", location, "\n")
    return(NULL)
  })
  
  if (is.null(bootstrap_ci)) {
    cat("Skipping location:", location, "due to confidence interval calculation error.\n")
    concentration_results <- concentration_results %>%
      add_row(
        location_name = location,
        concentration_index = NA,
        ci_lower = NA,
        ci_upper = NA,
        total_points = total_points,
        valid_points = valid_points
      )
    locations_failed <- c(locations_failed, location)
    next
  }
  
  concentration_index <- concentration_index_function(data, 1:nrow(data))
  
  concentration_results <- concentration_results %>%
    add_row(
      location_name = location,
      concentration_index = concentration_index,
      ci_lower = bootstrap_ci$percent[4],
      ci_upper = bootstrap_ci$percent[5],
      total_points = total_points,
      valid_points = valid_points
    )
  
  # Calculate cumulative proportions for plotting the concentration curve
  sorted_indices <- order(total_attention_scores)
  cumulative_attention_scores <- cumsum(total_attention_scores[sorted_indices]) / sum(total_attention_scores)
  cumulative_daly_values <- cumsum(daly_values[sorted_indices]) / sum(daly_values)
  
  # Plot the concentration curve
  plot(cumulative_daly_values, cumulative_attention_scores, type="l", col="blue", lwd=2, 
       xlab="Cumulative Proportion of DALY Values", ylab="Cumulative Proportion of Attention Scores",
       main=paste("Concentration Curve -", location))
  abline(0, 1, col="red", lwd=2, lty=2) # Line of equality
  
  cat("Displayed plot for location:", location, "\n")
  locations_passed <- c(locations_passed, location)
}

# Print summary of results
print(concentration_results)
cat("Locations passed:", paste(locations_passed, collapse = ", "), "\n")
cat("Locations failed:", paste(locations_failed, collapse = ", "), "\n")

# Combined concentration curve for all data
all_attention_scores <- combined_dataset$total_attention_score
all_daly_values <- combined_dataset$DALY

# Remove rows with missing values
valid_data_all <- combined_dataset %>%
  filter(!is.na(total_attention_score) & !is.na(DALY))

all_attention_scores <- valid_data_all$total_attention_score
all_daly_values <- valid_data_all$DALY

# Prepare data for the combined plot
all_data <- data.frame(attention_scores = all_attention_scores, daly_values = all_daly_values)

# Calculate the concentration index and bootstrap CI for all data
all_concentration_index <- concentration_index_function(all_data, 1:nrow(all_data))

set.seed(123)  # for reproducibility
all_bootstrap_results <- boot(all_data, concentration_index_function, R = 1000)

# Calculate the 95% confidence interval for the combined data
all_bootstrap_ci <- boot.ci(all_bootstrap_results, type = "perc")

# Calculate cumulative proportions for plotting the combined concentration curve
sorted_indices_all <- order(all_attention_scores)
cumulative_attention_scores_all <- cumsum(all_attention_scores[sorted_indices_all]) / sum(all_attention_scores)
cumulative_daly_values_all <- cumsum(all_daly_values[sorted_indices_all]) / sum(all_daly_values)

# Plot the combined concentration curve
plot(cumulative_daly_values_all, cumulative_attention_scores_all, type="l", col="blue", lwd=2, 
     xlab="Cumulative Proportion of DALY Values", ylab="Cumulative Proportion of Attention Scores",
     main="Combined Concentration Curve")
abline(0, 1, col="red", lwd=2, lty=2) # Line of equality

# Print the combined concentration index and its confidence interval
cat("Combined Concentration Index:", all_concentration_index, "\n")
cat("95% Confidence Interval:", all_bootstrap_ci$percent[4], "-", all_bootstrap_ci$percent[5], "\n")


# Remove rows with NA values
concentration_results_clean <- na.omit(concentration_results)

# Plot concentration index with 95% confidence intervals for all locations individually and combined
ggplot(concentration_results_clean, aes(x = reorder(location_name, concentration_index), y = concentration_index)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
  coord_flip() +
  labs(
    title = "Concentration Index by Location",
    x = "Location",
    y = "Concentration Index"
  ) +
  theme_minimal()
```
```{r}

```

